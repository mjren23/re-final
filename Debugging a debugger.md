
## Debugging a debugger:

 <center> <font size="+2">Abstract</font></center>

*GDB* is a popular debugging tool used to investigate a programs execution and address space. As
debugging has grown in usage, CPU architectures have developed infrastructure to handle
debugging at a hardware level to allow for faster breakpoints with lower overhead. GDB’s usage
of these breakpoints, however, appears to be lacking some critical abilities to persist the CPU’s
debugging state across context switches and across various CPUs.

Most notably, in syscalls and signal handlers, addresses which are written by the kernel are
ignored by the user-land GDB watchpoints. We propose a means of fixing this issue by patching
GDB’s implementation of hardware watchpoints to allow for persistence of the debugging state.
Such design overhauls have been discussed by researchers over the last decade, and we hope to
expand upon these ideas to provide users the ability to investigate memory reliably and
accurately without needing a deep knowledge of the context switches involved in program
execution.

In this paper, we will investigate why GDB maintains this behavior while also developing
an understanding of how debuggers at-large function. We will illustrate a series of patches which attempted to allow for a general user population to set breakpoints and
watchpoints on memory with assurance that any read, write, or execution of that memory
(whether in kernel or userland) results in a break in GDB’s run. 

 <center> <font size="+2">Background</font></center>


**How a debugger works**

Given the extensive memory access a debugger requires, userland services are not sufficient to build a worthwhile debugger. For this reason, most modern operating systems offer a library of system calls intended for *tracing* the execution of a program along with its memory contents. 

In linux, the ```ptrace()``` function is responsible for attaching one process (the *tracer*) to another process (the *tracee*) for the purposes of debugging and memory examination/manipulation.

The ```ptrace()``` system call works by redirecting signals/interrupts generated by the *tracee* to be handled by the *tracer*, forcing the halting of execution of the *tracee* and allowing the *tracer* to interact with the kernel to examine and change the memory contents of the *tracee*. Then, the *tracer* can end the signal handling and hand execution back to the *tracee*.

*GDB* and other debuggers make extensive use of this system call to step through and examine program execution. Effectively, *GDB* is a convenient, user-friendly wrapper for the ```ptrace()``` syscall which offers a dense suite of program analysis and adjustment tools.

**Software breakpoints/watchpoints**

One of *GDB*'s most useful features is the ability to set breakpoints in execution. Users can denote particular instruction addresses at which they want the program to halt. 

To accomplish this, *GDB* actually injects code into the program using ```ptrace()``` and replaces the line you are investigating with a command which will cause a signal and halt execution at the desired line.

Because this is implemented programmatically, the number of software breakpoints is theoretically unbounded (outside of memory restrictions).

However, this forced halting (known as a *breakpoint*), only has the power to halt at the execution of a line and does not detect memory access to particular addresses. Thus, *GDB* offers a second type of halting (known as a *watchpoint*) which halts at memory access to a particular address. 

Similar to software breakpoints, watchpoints can be implemented programmatically by simply checking the state of the memory address after each line of execution, but this adds an absurd amount of overhead to the program and makes such an approach infeasible for computationally complex programs.

**Hardware breakpoints/watchpoints**

To combat this overhead, CPU architectures have began providing special debugging registers which allow for comparisons between addresses in the data bus and the addresses in the debug registers to be handled directly at the CPU. For this paper, we will investigate such features in the x86 architecture. 

In this case, 8 debug registers are provided by the CPU, but 2 of these are fully reserved by the architecture, so we will examine the remaining 6 registers.

![Intel Debug Registers](https://i.ibb.co/9VDBQdG/debug-registers.jpg)

<sup>Source: </sup><a href="https://www.intel.com/content/dam/support/us/en/documents/processors/pentium4/sb/253669.pdf"><sup>Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3B: System Programming Guide, Part 2</sup></a>


The first 4 registers (```DR0``` - ```DR3```) store the 32-bit addresses which indicate which addresses in memory we are setting a watchpoint on. This also illustrates one limitation with hardware watchpoints, they have a limited number specified by the CPU architecture. 

As noted, the next two registers (```DR4``` - ```DR5```) are reserved by the CPU.

The following two registers (```DR6``` - ```DR7```) are used to control the behavior of these watchpoints (the only control bits we will worry about are in ```DR7```). 

 <center> <font size="+2">The Problem with GDB</font></center>

**Local vs. Global Watchpoints**

Local watchpoints are set for a single task only, while global watchpoints persist across tasks. In essence, if a different program from the one setting a watchpoint accesses the same memory location, GDB will only break if global watchpoints are enabled. The Intel software developer's manual for 64 and IA-32 architectures describes the local flag as being cleared by the processor automatically "on every task switch to avoid unwanted breakpoint conditions in the new
task." As a potential spoiler for the rest of the paper, we assumed that this meant that a local flag would cause a watchpoint to be set on one core and only for one task, while setting the global flag would enable the watchpoint to be set across cores and tasks. 

**Proposed Solutions**

![enter image description here](https://drive.google.com/uc?export=view&id=18aQjc6S9Q0kv4QDwPJDSeXwp3m-jtuSm)

A 2009 paper by Prasad Krishnan discusses the incorporation of methods to set hardware breakpoints in the Linux kernel. The difference between a kernel hardware breakpoint and a user kernel breakpoint is depicted in the figure above, where kernel breakpoints are maintained across CPUs in multiprocessor systems, while user-space breakpoints are set on a single core when the thread they are operating on is scheduled. 

**Our Proposal**

 <center> <font size="+2">Patching GDB</font></center>

**Dynamic Analysis**

We wanted to figure out how GDB handles setting hardware watchpoints under the hood. To this end, we tried several methods of dynamic analysis to check A. what values are written to debug registers, and B. how they are being written. Our first thought was to use ```ptrace()``` to observe GDB as it was running. However, we discovered that  ```ptrace()``` cannot trace GDB as it itself traces a process (?). Next, we tried to attach GDB to a GDB process running on another file, which also ended up not working. As we were "GDB-ing GDB," we used GDB's maintenance commands to show the values of the debug registers, thinking that perhaps the values of the registers GDB maintained were different than the actual hardware. We examined the values and verified that they matched up with what we expected based on the Intel diagram (?). In general, dynamic analysis was marginally useful as a sanity check on exploring the contents of debug registers, but proved too finicky to be helpful past that. 

**Static Analysis**

GDB is an open source program, published by the GNU project, so we explored the source code to understand how it sets values in debug registers, with the hope of identifying files and sections relevant to our research question. We started by locating strings that GDB displays to the user upon setting a watchpoint (e.g. "Hardware watchpoint 2: 0x100000900"). This led us to the ```breakpoint.c``` file in GDB's source code, helping us understand the complex flow that GDB takes to implement a hardware watchpoint (or any watch/breakpoint). Next, we found a comment in ```x86_dregs.c``` explaining that GDB assumes only local watchpoints, but that the global enable flag could be set in DR7. As explained above, this would ensure that the watchpoint is maintained across context switches, rather than only for the process initially setting the watchpoint. GDB's developers write that it is theoretically possible to set global watchpoints and implement a macro to do so. 

We therefore attempted to modify the source code and rebuild the program to use the function for global watchpoints. This proved unusually challenging, and we ran into several problems with configuring and making the binary. To make a long and uninteresting story short, we discovered that this statement in the GDB documentation was patently false. 

![Screenshot of GDB documentation](https://drive.google.com/uc?export=view&id=1F2DebSTMk2zrBKoRkVQdfCy2K9hQqPl8)

We turned instead to directly editing the binary. 

**Updating the Binary**


 <center> <font size="+2">Back to the Drawing Board</font></center>

In hindsight, an immediate investigation of *GDB*'s handling of watchpoints was not the most effective method of analysis. After doing this investigation we decided that we should have illustrate that setting the ```global detect``` bit in ```DR7``` would have even produced the result we desired. This is to say, our goal from the outset was to get *GDB* to set this bit, but even with this bit set, the problem does not appear to be solved. 

To narrow down the issues we were facing, we wrote a short script that was intended to examine whether or not a watchpoint with ```global detect``` enabled would be able to detect kernel level changes. 

**Proof of Concept**

**The Takeaway**

As new reverse engineers, this is an important lesson in the flow of program analysis - narrow down the problem, then expand to a larger code base. The testing script should have been written long before *GDB*'s source code was examined as this would have saved us a great deal of time exploring a route that was to no avail. 

 <center> <font size="+2">Conclusion</font></center>
## Debugging a debugger:

 <center> <font size="+2">Abstract</font></center>

*GDB* is a popular debugging tool used to investigate a programs execution and address space. As
debugging has grown in usage, CPU architectures have developed infrastructure to handle
debugging at a hardware level to allow for faster breakpoints with lower overhead. GDB’s usage
of these breakpoints, however, appears to be lacking some critical abilities to persist the CPU’s
debugging state across context switches and across various CPUs.

Most notably, in syscalls and signal handlers, addresses which are written by the kernel are
ignored by the user-land GDB watchpoints. We propose a means of fixing this issue by patching
GDB’s implementation of hardware watchpoints to allow for persistence of the debugging state.
Such design overhauls have been discussed by researchers over the last decade, and we hope to
expand upon these ideas to provide users the ability to investigate memory reliably and
accurately without needing a deep knowledge of the context switches involved in program
execution.

In this paper, we will investigate why GDB maintains this behavior while also developing
an understanding of how debuggers at-large function. We will illustrate a series of patches which attempted to allow for a general user population to set breakpoints and
watchpoints on memory with assurance that any read, write, or execution of that memory
(whether in kernel or userland) results in a break in GDB’s run. 

 <center> <font size="+2">Background</font></center>


**How a debugger works**

Given the exstensive memory access a debugger requires, userland services are not sufficient to build a worthwhile debugger. For this reason, most modern operating systems offer a library of system calls intended for *tracing* the execution of a program along with its memory contents. 

In linux, the ```ptrace()``` function is responsible for attaching one process (the *tracer*) to another process (the *tracee*) for the purposes of debugging and memory examination/manipulation.

The ```ptrace()``` system call works by redirecting signals/interrupts generated by the *tracee* to be handled by the *tracer*, forcing the halting of execution of the *tracee* and allowing the *tracer* to interact with the kernel to examine and change the memory contents of the *tracee*. Then, the *tracer* can end the signal handling and hand execution back to the *tracee*.

*GDB* and other debuggers make extensive use of this system call to step through and examine program execution. Effectively, *GDB* is a convenient, user-friendly wrapper for the ```ptrace()``` syscall which offers a dense suite of program analysis and adjustment tools.

**Software breakpoints/watchpoints**

One of *GDB*'s most useful features is the ability to set breakpoints in execution. Users can denote particular instruction addresses at which they want the program to halt. 

To accomplish this, *GDB* actually injects code into the program using ```ptrace()``` and replaces the line you are investigating with a command which will cause a signal and halt execution at the desired line.

Because this is implemented programattically, the number of software breakpoints is theoretically unbounded (outside of memory restrictions).

However, this forced halting (known as a *breakpoint*), only has the power to halt at the execution of a line and does not detect memory access to particular addresses. Thus, *GDB* offers a second type of halting (known as a *watchpoint*) which halts at memory access to a particular address. 

Similar to software breakpoints, watchpoints can be implemented programatically by simply checking the state of the memory address after each line of execution, but this adds an absurd amount of overhead to the program and makes such an approach infeasible for computationally complex programs.

**Hardware breakpoints/watchpoints**

To combat this overhead, CPU architecures have began providing special debugging registers which allow for comparisons between addresses in the data bus and the addresses in the debug registers to be handled directly at the CPU. For this paper, we will investigate such features in the x86 architecture. 

In this case, 8 debug registers are provided by the CPU, but 2 of these are fully reserved by the architecture, so we will examine the remainign 6 registers.

![Intel Debug Registers](https://i.ibb.co/9VDBQdG/debug-registers.jpg)

<sup>Source: </sup><a href="https://www.intel.com/content/dam/support/us/en/documents/processors/pentium4/sb/253669.pdf"><sup>Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3B: System Programming Guide, Part 2</sup></a>


The first 4 registers (```DR0``` - ```DR3```) store the 32-bit addresses which indicate which addresses in memory we are setting a watchpoint on. This also illustrates one limitation with hardware watchpoints, they have a limited number specified by the CPU architecture. 

As noted, the next two registerrs (```DR4``` - ```DR5```) are reserved by the CPU.

The following two registers (```DR6``` - ```DR7```) are used to control the behavior of these watchpoints (the only control bits we will worry about are in ```DR7```). 

 <center> <font size="+2">The Problem with GDB</font></center>

**Local vs. Global Watchpoints**

**Proposed Solutions**

**Our Proposal**

 <center> <font size="+2">Patching GDB</font></center>

**Dynamic Analysis**

**Static Analsysis**

**Updating the Binary**


 <center> <font size="+2">Back to the Drawing Board</font></center>

In hindsight, an immediate investigation of *GDB*'s handling of watchpoints was not the most effective method of analysis. After doing this investigation we decided that we should have illustrate that setting the ```global detect``` bit in ```DR7``` would have even produced the result we desired. This is to say, our goal from the outset was to get *GDB* to set this bit, but even with this bit set, the problem does not appear to be solved. 

To narrow down the issues we were facing, we wrote a short script that was intended to examine whether or not a watchpoint with ```global detect``` enabled would be able to detect kernel level changes. 

**Proof of Concept**

**The Takeaway**

As new reverse engineers, this is an important lesson in the flow of program analysis - narrow down the problem, then expand to a larger code base. The testing script should have been written long before *GDB*'s source code was examined as this would have saved us a great deal of time exploring a route that was to no avail. 

 <center> <font size="+2">Conclusion</font></center>
